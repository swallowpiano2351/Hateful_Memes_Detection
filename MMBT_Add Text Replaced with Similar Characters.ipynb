{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_7LwHf634R4",
        "outputId": "0dd70655-9c34-4121-dbaa-b4f92442cd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 01:51:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRZNS8cJ4USq",
        "outputId": "43448589-4376-4cc6-cddd-d690cf371ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I_ggRJpiDMVs",
        "outputId": "dcf4b934-6d4b-4743-cdb5-bde80d450673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mmf'...\n",
            "remote: Enumerating objects: 22960, done.\u001b[K\n",
            "remote: Counting objects: 100% (2903/2903), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1140/1140), done.\u001b[K\n",
            "remote: Total 22960 (delta 1909), reused 2381 (delta 1483), pack-reused 20057\u001b[K\n",
            "Receiving objects: 100% (22960/22960), 17.03 MiB | 21.32 MiB/s, done.\n",
            "Resolving deltas: 100% (14725/14725), done.\n",
            "/content/mmf\n",
            "Obtaining file:///content/mmf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 5b9995da0) to /tmp/pip-install-0n8kc_1x/pytorch-lightning_b34876598adc4608b61cfbf1443ead45\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-0n8kc_1x/pytorch-lightning_b34876598adc4608b61cfbf1443ead45\n",
            "\u001b[33m  WARNING: Did not find branch or tag '5b9995da0', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 5b9995da0\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting GitPython==3.1.0\n",
            "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
            "\u001b[K     |████████████████████████████████| 450 kB 14.7 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 71.1 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting iopath==0.1.8\n",
            "  Downloading iopath-0.1.8-py3-none-any.whl (19 kB)\n",
            "Collecting torchvision<=0.10.0,>=0.7.0\n",
            "  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Collecting matplotlib==3.3.4\n",
            "  Downloading matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 30.0 MB/s \n",
            "\u001b[?25hCollecting omegaconf<=2.1,>=2.0.6\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
            "Collecting lmdb==0.98\n",
            "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
            "\u001b[K     |████████████████████████████████| 869 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting ftfy==5.8\n",
            "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting torchaudio<=0.9.0,>=0.6.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Collecting torch<=1.9.0,>=1.6.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.6 kB/s \n",
            "\u001b[?25hCollecting pillow==8.3.1\n",
            "  Downloading Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (5.4.8)\n",
            "Collecting transformers<=4.10.1,>=3.4.0\n",
            "  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 57.9 MB/s \n",
            "\u001b[?25hCollecting pycocotools==2.0.2\n",
            "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
            "Collecting fasttext==0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (21.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (2.7.0)\n",
            "Collecting typing-extensions>=4.0.0\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 82.1 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (4.8.2)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 67.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.42.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1->mmf==1.0.0rc12) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 658 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (2.0.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
            "Building wheels for collected packages: pytorch-lightning, fasttext, ftfy, lmdb, nltk, pycocotools, future, antlr4-python3-runtime\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.6.0.dev0-py3-none-any.whl size=520529 sha256=0f9b65ec347378ac9f14c2ae0109ab719eb319eac1c97a3d217e638b1425710c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w45m_lpa/wheels/d2/d4/53/7897e6b8eead01814632c6795a3dab497338a90bce62eae705\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2491227 sha256=29d7639ce2d730a2cf1d50d540784ecbeb295c705e8a486adbef018a2c86b9cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45633 sha256=db07b6c585d3407c88fd91b12ffdda630b64083d67768c1e7055e45139777ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/1c/fc/8b19700f939810cd8fd9495ae34934b246279791288eda1c31\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219742 sha256=560f6e03fbabf34748174564c363830f6d391f633e7bb796aa9b6fd376290db8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449920 sha256=24aad4d8d6c5d16420eb707ab002a92a8883485560c66688858ecaad20549068\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=264000 sha256=482d703d78755ba68fc3310600e9bc87974a19cbc3aaa8a1273e17e09b79e2b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=20ace4421c422bc63912f9c69703bbcf8b714ac28c92292a7c17d819c5057689\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=73d9b60c642c634af98253784414cbc3e57a308fa2fd895098239aeb29fcd43b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built pytorch-lightning fasttext ftfy lmdb nltk pycocotools future antlr4-python3-runtime\n",
            "Installing collected packages: typing-extensions, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, tqdm, torch, smmap, PyYAML, pillow, fsspec, aiohttp, xxhash, torchmetrics, tokenizers, sentencepiece, sacremoses, pyDeprecate, pybind11, portalocker, matplotlib, huggingface-hub, gitdb, future, antlr4-python3-runtime, transformers, torchvision, torchtext, torchaudio, pytorch-lightning, pycocotools, omegaconf, nltk, lmdb, iopath, GitPython, ftfy, fasttext, datasets, mmf\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.3\n",
            "    Uninstalling pycocotools-2.0.3:\n",
            "      Successfully uninstalled pycocotools-2.0.3\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Running setup.py develop for mmf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.0 PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.2.1 fasttext-0.9.1 frozenlist-1.2.0 fsspec-2021.11.1 ftfy-5.8 future-0.18.2 gitdb-4.0.9 huggingface-hub-0.2.1 iopath-0.1.8 lmdb-0.98 matplotlib-3.3.4 mmf-1.0.0rc12 multidict-5.2.0 nltk-3.4.5 omegaconf-2.1.0 pillow-8.3.1 portalocker-2.3.2 pyDeprecate-0.3.1 pybind11-2.8.1 pycocotools-2.0.2 pytorch-lightning-1.6.0.dev0 sacremoses-0.0.46 sentencepiece-0.1.96 smmap-5.0.0 tokenizers-0.10.3 torch-1.9.0 torchaudio-0.9.0 torchmetrics-0.6.1 torchtext-0.5.0 torchvision-0.10.0 tqdm-4.49.0 transformers-4.10.1 typing-extensions-4.0.1 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/mmf.git\n",
        "%cd mmf\n",
        "!pip install --editable ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAvGPAoIVSY4",
        "outputId": "fd14fb44-88b4-48ec-e7ac-c2c8ccb76823"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is /content/drive/MyDrive/hateful_memes.zip\n",
            "Copying /content/drive/MyDrive/hateful_memes.zip\n",
            "Unzipping /content/drive/MyDrive/hateful_memes.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ]
        }
      ],
      "source": [
        "!mmf_convert_hm --zip_file /content/drive/MyDrive/hateful_memes.zip --password ys --bypass_checksum=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUFxEShubtGL",
        "outputId": "5e1dcf55-7784-4590-a9fb-8ab0a1248da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (3.3.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (8.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.4) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Downloading extras.tar.gz: 0.00B [00:00, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading extras.tar.gz: 100%|██████████| 211k/211k [00:01<00:00, 148kB/s] \n",
            "/root/.cache/torch/mmf/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Starting checksum for extras.tar.gz]\n",
            "[ Checksum successful for extras.tar.gz]\n",
            "Unpacking extras.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/mmf/glove.6B.zip: 862MB [02:42, 5.30MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:37<00:00, 10645.08it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib==3.3.4\n",
        "from mmf.common.registry import registry\n",
        "from mmf.models.mmbt import MMBT\n",
        "from mmf.utils.build import build_dataset\n",
        "from mmf.utils.env import setup_imports\n",
        "\n",
        "setup_imports()\n",
        "dataset = build_dataset(\"hateful_memes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANjYvkMuaC6g",
        "outputId": "0c1d1e88-9150-490e-a1bd-c7231800a4e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/mmbt/defaults.yaml\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 5000\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'training.max_updates=5000'])\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf_cli.run: \u001b[0mUsing seed 6113052\n",
            "\u001b[32m2021-12-11T02:05:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf6zlh0c4\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 24.4kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbtloflvk\n",
            "Downloading: 100% 570/570 [00:00<00:00, 484kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxxo_ch9m\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 679kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpppada_ts\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.37MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:05:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:05:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T02:05:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-11T02:05:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-11T02:05:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-11T02:05:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2f3ri8a7\n",
            "Downloading: 100% 440M/440M [00:07<00:00, 57.0MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100% 230M/230M [00:02<00:00, 91.7MB/s]\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2021-12-11T02:05:33 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:05:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:05:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\n",
            "\u001b[32m2021-12-11T02:07:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/5000, train/hateful_memes/cross_entropy: 0.7035, train/hateful_memes/cross_entropy/avg: 0.7035, train/total_loss: 0.7035, train/total_loss/avg: 0.7035, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 5000, lr: 0., ups: 1.09, time: 01m 32s 839ms, time_since_start: 01m 32s 927ms, eta: 01h 17m 06s 494ms\n",
            "\u001b[32m2021-12-11T02:08:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/5000, train/hateful_memes/cross_entropy: 0.6472, train/hateful_memes/cross_entropy/avg: 0.6754, train/total_loss: 0.6472, train/total_loss/avg: 0.6754, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 622ms, time_since_start: 03m 02s 549ms, eta: 01h 12m 55s 028ms\n",
            "\u001b[32m2021-12-11T02:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/5000, train/hateful_memes/cross_entropy: 0.6472, train/hateful_memes/cross_entropy/avg: 0.6655, train/total_loss: 0.6472, train/total_loss/avg: 0.6655, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 527ms, time_since_start: 04m 32s 077ms, eta: 01h 11m 19s 320ms\n",
            "\u001b[32m2021-12-11T02:11:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/5000, train/hateful_memes/cross_entropy: 0.6472, train/hateful_memes/cross_entropy/avg: 0.6672, train/total_loss: 0.6472, train/total_loss/avg: 0.6672, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 658ms, time_since_start: 06m 02s 735ms, eta: 01h 10m 41s 186ms\n",
            "\u001b[32m2021-12-11T02:13:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/5000, train/hateful_memes/cross_entropy: 0.6472, train/hateful_memes/cross_entropy/avg: 0.6606, train/total_loss: 0.6472, train/total_loss/avg: 0.6606, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 620ms, time_since_start: 07m 32s 356ms, eta: 01h 08m 21s 473ms\n",
            "\u001b[32m2021-12-11T02:14:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/5000, train/hateful_memes/cross_entropy: 0.6459, train/hateful_memes/cross_entropy/avg: 0.6502, train/total_loss: 0.6459, train/total_loss/avg: 0.6502, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 457ms, time_since_start: 09m 01s 814ms, eta: 01h 06m 43s 059ms\n",
            "\u001b[32m2021-12-11T02:16:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/5000, train/hateful_memes/cross_entropy: 0.6459, train/hateful_memes/cross_entropy/avg: 0.6391, train/total_loss: 0.6459, train/total_loss/avg: 0.6391, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 558ms, time_since_start: 10m 31s 372ms, eta: 01h 05m 16s 467ms\n",
            "\u001b[32m2021-12-11T02:17:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/5000, train/hateful_memes/cross_entropy: 0.6344, train/hateful_memes/cross_entropy/avg: 0.6182, train/total_loss: 0.6344, train/total_loss/avg: 0.6182, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 867ms, time_since_start: 12m 02s 239ms, eta: 01h 04m 41s 299ms\n",
            "\u001b[32m2021-12-11T02:19:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/5000, train/hateful_memes/cross_entropy: 0.6344, train/hateful_memes/cross_entropy/avg: 0.6122, train/total_loss: 0.6344, train/total_loss/avg: 0.6122, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 900, iterations: 900, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 692ms, time_since_start: 13m 31s 932ms, eta: 01h 02m 19s 917ms\n",
            "\u001b[32m2021-12-11T02:20:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-11T02:20:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:20:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:21:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:21:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/5000, train/hateful_memes/cross_entropy: 0.6133, train/hateful_memes/cross_entropy/avg: 0.6123, train/total_loss: 0.6133, train/total_loss/avg: 0.6123, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 1000, iterations: 1000, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 02m 01s 111ms, time_since_start: 15m 33s 043ms, eta: 01h 22m 06s 827ms\n",
            "\u001b[32m2021-12-11T02:21:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-11T02:21:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:21:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:21:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T02:21:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-11T02:21:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T02:21:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:21:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-12-11T02:21:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:22:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:22:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/5000, val/hateful_memes/cross_entropy: 0.7339, val/total_loss: 0.7339, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.3618, val/hateful_memes/roc_auc: 0.6036, num_updates: 1000, epoch: 3, iterations: 1000, max_updates: 5000, val_time: 01m 07s 724ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.603574\n",
            "\u001b[32m2021-12-11T02:23:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/5000, train/hateful_memes/cross_entropy: 0.6133, train/hateful_memes/cross_entropy/avg: 0.6034, train/total_loss: 0.6133, train/total_loss/avg: 0.6034, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 455ms, time_since_start: 18m 11s 224ms, eta: 59m 47s 739ms\n",
            "\u001b[32m2021-12-11T02:25:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/5000, train/hateful_memes/cross_entropy: 0.5980, train/hateful_memes/cross_entropy/avg: 0.5943, train/total_loss: 0.5980, train/total_loss/avg: 0.5943, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1200, iterations: 1200, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 901ms, time_since_start: 19m 42s 126ms, eta: 58m 32s 980ms\n",
            "\u001b[32m2021-12-11T02:26:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/5000, train/hateful_memes/cross_entropy: 0.5980, train/hateful_memes/cross_entropy/avg: 0.5653, train/total_loss: 0.5980, train/total_loss/avg: 0.5653, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1300, iterations: 1300, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 465ms, time_since_start: 21m 11s 591ms, eta: 56m 06s 495ms\n",
            "\u001b[32m2021-12-11T02:28:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/5000, train/hateful_memes/cross_entropy: 0.5722, train/hateful_memes/cross_entropy/avg: 0.5606, train/total_loss: 0.5722, train/total_loss/avg: 0.5606, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1400, iterations: 1400, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 494ms, time_since_start: 22m 41s 086ms, eta: 54m 36s 577ms\n",
            "\u001b[32m2021-12-11T02:29:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/5000, train/hateful_memes/cross_entropy: 0.5722, train/hateful_memes/cross_entropy/avg: 0.5459, train/total_loss: 0.5722, train/total_loss/avg: 0.5459, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1500, iterations: 1500, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 531ms, time_since_start: 24m 10s 617ms, eta: 53m 06s 870ms\n",
            "\u001b[32m2021-12-11T02:31:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/5000, train/hateful_memes/cross_entropy: 0.5640, train/hateful_memes/cross_entropy/avg: 0.5348, train/total_loss: 0.5640, train/total_loss/avg: 0.5348, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1600, iterations: 1600, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 774ms, time_since_start: 25m 41s 392ms, eta: 52m 18s 810ms\n",
            "\u001b[32m2021-12-11T02:32:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/5000, train/hateful_memes/cross_entropy: 0.5640, train/hateful_memes/cross_entropy/avg: 0.5274, train/total_loss: 0.5640, train/total_loss/avg: 0.5274, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1700, iterations: 1700, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 455ms, time_since_start: 27m 10s 847ms, eta: 50m 02s 204ms\n",
            "\u001b[32m2021-12-11T02:34:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/5000, train/hateful_memes/cross_entropy: 0.5145, train/hateful_memes/cross_entropy/avg: 0.5170, train/total_loss: 0.5145, train/total_loss/avg: 0.5170, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1800, iterations: 1800, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 493ms, time_since_start: 28m 40s 341ms, eta: 48m 32s 482ms\n",
            "\u001b[32m2021-12-11T02:35:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/5000, train/hateful_memes/cross_entropy: 0.5145, train/hateful_memes/cross_entropy/avg: 0.4996, train/total_loss: 0.5145, train/total_loss/avg: 0.4996, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1900, iterations: 1900, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 488ms, time_since_start: 30m 09s 829ms, eta: 47m 01s 297ms\n",
            "\u001b[32m2021-12-11T02:37:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-11T02:37:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:37:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:37:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:37:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, train/hateful_memes/cross_entropy: 0.4991, train/hateful_memes/cross_entropy/avg: 0.4796, train/total_loss: 0.4991, train/total_loss/avg: 0.4796, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 2000, iterations: 2000, max_updates: 5000, lr: 0.00001, ups: 0.81, time: 02m 03s 521ms, time_since_start: 32m 13s 351ms, eta: 01h 02m 48s 648ms\n",
            "\u001b[32m2021-12-11T02:37:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-11T02:37:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:37:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:37:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T02:37:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-11T02:37:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T02:37:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:38:09 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-12-11T02:38:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:38:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:38:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, val/hateful_memes/cross_entropy: 0.8790, val/total_loss: 0.8790, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.4906, val/hateful_memes/roc_auc: 0.6583, num_updates: 2000, epoch: 6, iterations: 2000, max_updates: 5000, val_time: 01m 06s 565ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.658265\n",
            "\u001b[32m2021-12-11T02:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/5000, train/hateful_memes/cross_entropy: 0.4940, train/hateful_memes/cross_entropy/avg: 0.4592, train/total_loss: 0.4940, train/total_loss/avg: 0.4592, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 2100, iterations: 2100, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 250ms, time_since_start: 34m 50s 169ms, eta: 44m 21s 762ms\n",
            "\u001b[32m2021-12-11T02:41:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/5000, train/hateful_memes/cross_entropy: 0.4720, train/hateful_memes/cross_entropy/avg: 0.4456, train/total_loss: 0.4720, train/total_loss/avg: 0.4456, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 2200, iterations: 2200, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 520ms, time_since_start: 36m 19s 690ms, eta: 42m 29s 200ms\n",
            "\u001b[32m2021-12-11T02:43:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/5000, train/hateful_memes/cross_entropy: 0.4091, train/hateful_memes/cross_entropy/avg: 0.4296, train/total_loss: 0.4091, train/total_loss/avg: 0.4296, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 2300, iterations: 2300, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 424ms, time_since_start: 37m 49s 114ms, eta: 40m 55s 509ms\n",
            "\u001b[32m2021-12-11T02:44:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/5000, train/hateful_memes/cross_entropy: 0.3686, train/hateful_memes/cross_entropy/avg: 0.4194, train/total_loss: 0.3686, train/total_loss/avg: 0.4194, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 2400, iterations: 2400, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 715ms, time_since_start: 39m 19s 829ms, eta: 39m 58s 693ms\n",
            "\u001b[32m2021-12-11T02:46:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/5000, train/hateful_memes/cross_entropy: 0.3408, train/hateful_memes/cross_entropy/avg: 0.4031, train/total_loss: 0.3408, train/total_loss/avg: 0.4031, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 2500, iterations: 2500, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 529ms, time_since_start: 40m 49s 359ms, eta: 37m 56s 280ms\n",
            "\u001b[32m2021-12-11T02:47:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/5000, train/hateful_memes/cross_entropy: 0.3399, train/hateful_memes/cross_entropy/avg: 0.3947, train/total_loss: 0.3399, train/total_loss/avg: 0.3947, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 2600, iterations: 2600, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 496ms, time_since_start: 42m 18s 856ms, eta: 36m 24s 443ms\n",
            "\u001b[32m2021-12-11T02:49:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/5000, train/hateful_memes/cross_entropy: 0.2182, train/hateful_memes/cross_entropy/avg: 0.3811, train/total_loss: 0.2182, train/total_loss/avg: 0.3811, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 2700, iterations: 2700, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 548ms, time_since_start: 43m 48s 404ms, eta: 34m 54s 633ms\n",
            "\u001b[32m2021-12-11T02:50:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/5000, train/hateful_memes/cross_entropy: 0.1856, train/hateful_memes/cross_entropy/avg: 0.3680, train/total_loss: 0.1856, train/total_loss/avg: 0.3680, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2800, iterations: 2800, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 897ms, time_since_start: 45m 19s 302ms, eta: 33m 53s 751ms\n",
            "\u001b[32m2021-12-11T02:52:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/5000, train/hateful_memes/cross_entropy: 0.1850, train/hateful_memes/cross_entropy/avg: 0.3557, train/total_loss: 0.1850, train/total_loss/avg: 0.3557, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2900, iterations: 2900, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 451ms, time_since_start: 46m 48s 754ms, eta: 31m 50s 418ms\n",
            "\u001b[32m2021-12-11T02:53:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-11T02:53:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:54:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:54:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:54:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/5000, train/hateful_memes/cross_entropy: 0.1832, train/hateful_memes/cross_entropy/avg: 0.3441, train/total_loss: 0.1832, train/total_loss/avg: 0.3441, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 3000, iterations: 3000, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 02m 977ms, time_since_start: 48m 49s 731ms, eta: 41m 675ms\n",
            "\u001b[32m2021-12-11T02:54:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-11T02:54:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:54:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T02:54:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T02:54:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-11T02:54:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T02:54:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T02:54:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T02:55:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T02:55:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/5000, val/hateful_memes/cross_entropy: 1.7234, val/total_loss: 1.7234, val/hateful_memes/accuracy: 0.6593, val/hateful_memes/binary_f1: 0.3907, val/hateful_memes/roc_auc: 0.6308, num_updates: 3000, epoch: 8, iterations: 3000, max_updates: 5000, val_time: 46s 373ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.658265\n",
            "\u001b[32m2021-12-11T02:56:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/5000, train/hateful_memes/cross_entropy: 0.1599, train/hateful_memes/cross_entropy/avg: 0.3335, train/total_loss: 0.1599, train/total_loss/avg: 0.3335, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 3100, iterations: 3100, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 544ms, time_since_start: 51m 06s 651ms, eta: 29m 09s 589ms\n",
            "\u001b[32m2021-12-11T02:58:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/5000, train/hateful_memes/cross_entropy: 0.1004, train/hateful_memes/cross_entropy/avg: 0.3232, train/total_loss: 0.1004, train/total_loss/avg: 0.3232, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 3200, iterations: 3200, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 822ms, time_since_start: 52m 37s 473ms, eta: 27m 42s 603ms\n",
            "\u001b[32m2021-12-11T02:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/5000, train/hateful_memes/cross_entropy: 0.0775, train/hateful_memes/cross_entropy/avg: 0.3144, train/total_loss: 0.0775, train/total_loss/avg: 0.3144, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 3300, iterations: 3300, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 427ms, time_since_start: 54m 06s 901ms, eta: 25m 46s 112ms\n",
            "\u001b[32m2021-12-11T03:01:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/5000, train/hateful_memes/cross_entropy: 0.0504, train/hateful_memes/cross_entropy/avg: 0.3053, train/total_loss: 0.0504, train/total_loss/avg: 0.3053, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 3400, iterations: 3400, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 588ms, time_since_start: 55m 36s 489ms, eta: 24m 17s 783ms\n",
            "\u001b[32m2021-12-11T03:02:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/5000, train/hateful_memes/cross_entropy: 0.0340, train/hateful_memes/cross_entropy/avg: 0.2967, train/total_loss: 0.0340, train/total_loss/avg: 0.2967, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 3500, iterations: 3500, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 556ms, time_since_start: 57m 06s 045ms, eta: 22m 46s 177ms\n",
            "\u001b[32m2021-12-11T03:04:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/5000, train/hateful_memes/cross_entropy: 0.0340, train/hateful_memes/cross_entropy/avg: 0.2894, train/total_loss: 0.0340, train/total_loss/avg: 0.2894, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 3600, iterations: 3600, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 075ms, time_since_start: 58m 37s 121ms, eta: 21m 36s 728ms\n",
            "\u001b[32m2021-12-11T03:05:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/5000, train/hateful_memes/cross_entropy: 0.0289, train/hateful_memes/cross_entropy/avg: 0.2816, train/total_loss: 0.0289, train/total_loss/avg: 0.2816, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 3700, iterations: 3700, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 466ms, time_since_start: 01h 06s 587ms, eta: 19m 42s 837ms\n",
            "\u001b[32m2021-12-11T03:07:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/5000, train/hateful_memes/cross_entropy: 0.0161, train/hateful_memes/cross_entropy/avg: 0.2743, train/total_loss: 0.0161, train/total_loss/avg: 0.2743, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 3800, iterations: 3800, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 451ms, time_since_start: 01h 01m 36s 039ms, eta: 18m 11s 667ms\n",
            "\u001b[32m2021-12-11T03:08:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/5000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2673, train/total_loss: 0.0148, train/total_loss/avg: 0.2673, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 3900, iterations: 3900, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 441ms, time_since_start: 01h 03m 05s 480ms, eta: 16m 40s 576ms\n",
            "\u001b[32m2021-12-11T03:10:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-11T03:10:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T03:10:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T03:10:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T03:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/5000, train/hateful_memes/cross_entropy: 0.0125, train/hateful_memes/cross_entropy/avg: 0.2606, train/total_loss: 0.0125, train/total_loss/avg: 0.2606, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 4000, iterations: 4000, max_updates: 5000, lr: 0., ups: 0.82, time: 02m 02s 101ms, time_since_start: 01h 05m 07s 582ms, eta: 20m 41s 776ms\n",
            "\u001b[32m2021-12-11T03:10:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-11T03:10:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:10:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:10:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T03:10:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-11T03:10:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T03:10:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T03:11:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T03:11:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T03:11:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/5000, val/hateful_memes/cross_entropy: 2.1033, val/total_loss: 2.1033, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4268, val/hateful_memes/roc_auc: 0.6470, num_updates: 4000, epoch: 11, iterations: 4000, max_updates: 5000, val_time: 46s 523ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.658265\n",
            "\u001b[32m2021-12-11T03:12:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/5000, train/hateful_memes/cross_entropy: 0.0108, train/hateful_memes/cross_entropy/avg: 0.2543, train/total_loss: 0.0108, train/total_loss/avg: 0.2543, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 4100, iterations: 4100, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 113ms, time_since_start: 01h 07m 24s 220ms, eta: 13m 44s 805ms\n",
            "\u001b[32m2021-12-11T03:14:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/5000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2484, train/total_loss: 0.0067, train/total_loss/avg: 0.2484, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 4200, iterations: 4200, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 569ms, time_since_start: 01h 08m 53s 790ms, eta: 12m 08s 734ms\n",
            "\u001b[32m2021-12-11T03:15:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/5000, train/hateful_memes/cross_entropy: 0.0060, train/hateful_memes/cross_entropy/avg: 0.2426, train/total_loss: 0.0060, train/total_loss/avg: 0.2426, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 4300, iterations: 4300, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 428ms, time_since_start: 01h 10m 23s 219ms, eta: 10m 36s 645ms\n",
            "\u001b[32m2021-12-11T03:17:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/5000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.2371, train/total_loss: 0.0041, train/total_loss/avg: 0.2371, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 4400, iterations: 4400, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 812ms, time_since_start: 01h 11m 54s 031ms, eta: 09m 14s 139ms\n",
            "\u001b[32m2021-12-11T03:18:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/5000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.2332, train/total_loss: 0.0041, train/total_loss/avg: 0.2332, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 4500, iterations: 4500, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 512ms, time_since_start: 01h 13m 23s 544ms, eta: 07m 35s 172ms\n",
            "\u001b[32m2021-12-11T03:20:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/5000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.2281, train/total_loss: 0.0035, train/total_loss/avg: 0.2281, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 4600, iterations: 4600, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 498ms, time_since_start: 01h 14m 53s 043ms, eta: 06m 04s 079ms\n",
            "\u001b[32m2021-12-11T03:21:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/5000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.2233, train/total_loss: 0.0031, train/total_loss/avg: 0.2233, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 4700, iterations: 4700, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 424ms, time_since_start: 01h 16m 22s 467ms, eta: 04m 32s 835ms\n",
            "\u001b[32m2021-12-11T03:23:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/5000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.2189, train/total_loss: 0.0031, train/total_loss/avg: 0.2189, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 4800, iterations: 4800, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 876ms, time_since_start: 01h 17m 53s 344ms, eta: 03m 04s 841ms\n",
            "\u001b[32m2021-12-11T03:24:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/5000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.2145, train/total_loss: 0.0024, train/total_loss/avg: 0.2145, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 4900, iterations: 4900, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 456ms, time_since_start: 01h 19m 22s 800ms, eta: 01m 30s 977ms\n",
            "\u001b[32m2021-12-11T03:26:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-11T03:26:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T03:26:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T03:26:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T03:26:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/5000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.2102, train/total_loss: 0.0013, train/total_loss/avg: 0.2102, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 5000, iterations: 5000, max_updates: 5000, lr: 0., ups: 0.83, time: 02m 707ms, time_since_start: 01h 21m 23s 508ms, eta: 0ms\n",
            "\u001b[32m2021-12-11T03:26:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-11T03:26:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:26:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:26:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-11T03:27:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-11T03:27:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T03:27:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-11T03:27:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-11T03:27:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-11T03:27:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/5000, val/hateful_memes/cross_entropy: 2.2185, val/total_loss: 2.2185, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.4051, val/hateful_memes/roc_auc: 0.6421, num_updates: 5000, epoch: 13, iterations: 5000, max_updates: 5000, val_time: 46s 997ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.658265\n",
            "\u001b[32m2021-12-11T03:27:44 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-12-11T03:27:44 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-12-11T03:27:44 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2021-12-11T03:27:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-12-11T03:27:55 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n",
            "\u001b[32m2021-12-11T03:27:55 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n",
            "\u001b[32m2021-12-11T03:27:55 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
            "\u001b[32m2021-12-11T03:27:58 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2021-12-11T03:27:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:27:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-11T03:27:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 63/63 [00:25<00:00,  2.47it/s]\n",
            "\u001b[32m2021-12-11T03:28:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n",
            "\u001b[32m2021-12-11T03:28:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-11T03:28:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, test/hateful_memes/cross_entropy: 0.8468, test/total_loss: 0.8468, test/hateful_memes/accuracy: 0.6590, test/hateful_memes/binary_f1: 0.5156, test/hateful_memes/roc_auc: 0.6819\n",
            "\u001b[32m2021-12-11T03:28:23 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 22m 50s 385ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!mmf_run config=projects/hateful_memes/configs/mmbt/defaults.yaml model=mmbt dataset=hateful_memes training.max_updates=5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE2XH-68VQNL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDhffPnizrL5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Meme.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_7LwHf634R4",
        "outputId": "563c6396-54b0-4a7a-b5a1-4e5a45cd5359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 23:55:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRZNS8cJ4USq",
        "outputId": "fbb48541-6f28-48c5-a1d5-45a1a231e975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I_ggRJpiDMVs",
        "outputId": "f102445c-5237-401f-851a-fefde8bda0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mmf'...\n",
            "remote: Enumerating objects: 22972, done.\u001b[K\n",
            "remote: Counting objects: 100% (2915/2915), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1148/1148), done.\u001b[K\n",
            "remote: Total 22972 (delta 1919), reused 2387 (delta 1487), pack-reused 20057\u001b[K\n",
            "Receiving objects: 100% (22972/22972), 17.04 MiB | 20.05 MiB/s, done.\n",
            "Resolving deltas: 100% (14735/14735), done.\n",
            "/content/mmf\n",
            "Obtaining file:///content/mmf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 5b9995da0) to /tmp/pip-install-o7mlg0xu/pytorch-lightning_a553c385d320414c90bf5f83ead54ade\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-o7mlg0xu/pytorch-lightning_a553c385d320414c90bf5f83ead54ade\n",
            "\u001b[33m  WARNING: Did not find branch or tag '5b9995da0', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 5b9995da0\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath==0.1.8\n",
            "  Downloading iopath-0.1.8-py3-none-any.whl (19 kB)\n",
            "Collecting fasttext==0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
            "Collecting torch<=1.9.0,>=1.6.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.6 kB/s \n",
            "\u001b[?25hCollecting pycocotools==2.0.2\n",
            "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
            "Collecting torchvision<=0.10.0,>=0.7.0\n",
            "  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting transformers<=4.10.1,>=3.4.0\n",
            "  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 23.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
            "Collecting torchaudio<=0.9.0,>=0.6.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting ftfy==5.8\n",
            "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (5.4.8)\n",
            "Collecting matplotlib==3.3.4\n",
            "  Downloading matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 18.0 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 28.3 MB/s \n",
            "\u001b[?25hCollecting GitPython==3.1.0\n",
            "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
            "\u001b[K     |████████████████████████████████| 450 kB 61.8 MB/s \n",
            "\u001b[?25hCollecting pillow==8.3.1\n",
            "  Downloading Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
            "Collecting omegaconf<=2.1,>=2.0.6\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting lmdb==0.98\n",
            "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
            "\u001b[K     |████████████████████████████████| 869 kB 53.5 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (2.7.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 69.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (21.3)\n",
            "Collecting typing-extensions>=4.0.0\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (4.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.4)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.11.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.37.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.42.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1->mmf==1.0.0rc12) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (3.1.1)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 702 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 52.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (2.0.8)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 70.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@5b9995da0->mmf==1.0.0rc12) (21.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.0.0)\n",
            "Building wheels for collected packages: pytorch-lightning, fasttext, ftfy, lmdb, nltk, pycocotools, future, antlr4-python3-runtime\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.6.0.dev0-py3-none-any.whl size=520529 sha256=c7d7748516813236c93d36db2c7ba49d8f0e4948274328a886ffb404c7a99790\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9wz_q8_o/wheels/d2/d4/53/7897e6b8eead01814632c6795a3dab497338a90bce62eae705\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2491265 sha256=56bdc96d15032af3527caf50ef03804577fdca592e2150f8850e99c3e7cd7494\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45633 sha256=adba00f8a85da7ffcd6491423228a7040d52b6bcafbf1018aa0ab73d2254535b\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/1c/fc/8b19700f939810cd8fd9495ae34934b246279791288eda1c31\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219740 sha256=9b214e81f57c2a5f2b0d4865d2ade57cbaf0e2b222a0b344d68f226287211dae\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449920 sha256=717645584623d1841a5d42f6bf9b85ba2d2d682bfda8a8afdcb2cc418133b89a\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=263999 sha256=9a6118e33cb4abc906766e3976aac9aaf3ca4883a31129a873354ae62936073b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=209f57d5e3a1711e25b4ed17dab53ee6fbbb36d9d1beae25cf47f00cdcf9df45\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=30f4a2484ef72cb71b174dd33bfef7b12bb311bfe9e46ea4d26bdb710bdcc605\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built pytorch-lightning fasttext ftfy lmdb nltk pycocotools future antlr4-python3-runtime\n",
            "Installing collected packages: typing-extensions, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, tqdm, torch, smmap, PyYAML, pillow, fsspec, aiohttp, xxhash, torchmetrics, tokenizers, sentencepiece, sacremoses, pyDeprecate, pybind11, portalocker, matplotlib, huggingface-hub, gitdb, future, antlr4-python3-runtime, transformers, torchvision, torchtext, torchaudio, pytorch-lightning, pycocotools, omegaconf, nltk, lmdb, iopath, GitPython, ftfy, fasttext, datasets, mmf\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.3\n",
            "    Uninstalling pycocotools-2.0.3:\n",
            "      Successfully uninstalled pycocotools-2.0.3\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Running setup.py develop for mmf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.0 PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.2.1 fasttext-0.9.1 frozenlist-1.2.0 fsspec-2021.11.1 ftfy-5.8 future-0.18.2 gitdb-4.0.9 huggingface-hub-0.2.1 iopath-0.1.8 lmdb-0.98 matplotlib-3.3.4 mmf-1.0.0rc12 multidict-5.2.0 nltk-3.4.5 omegaconf-2.1.0 pillow-8.3.1 portalocker-2.3.2 pyDeprecate-0.3.1 pybind11-2.8.1 pycocotools-2.0.2 pytorch-lightning-1.6.0.dev0 sacremoses-0.0.46 sentencepiece-0.1.96 smmap-5.0.0 tokenizers-0.10.3 torch-1.9.0 torchaudio-0.9.0 torchmetrics-0.6.1 torchtext-0.5.0 torchvision-0.10.0 tqdm-4.49.0 transformers-4.10.1 typing-extensions-4.0.1 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/mmf.git\n",
        "%cd mmf\n",
        "!pip install --editable ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAvGPAoIVSY4",
        "outputId": "442db883-bf93-4141-a396-38a95b9c6877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is /content/drive/MyDrive/hateful_memes.zip\n",
            "Copying /content/drive/MyDrive/hateful_memes.zip\n",
            "Unzipping /content/drive/MyDrive/hateful_memes.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ]
        }
      ],
      "source": [
        "!mmf_convert_hm --zip_file /content/drive/MyDrive/hateful_memes.zip --password ys --bypass_checksum=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUFxEShubtGL",
        "outputId": "44cad938-4689-4497-c814-af450de44704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (3.3.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (3.0.6)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (8.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.4) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Downloading extras.tar.gz: 0.00B [00:00, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading extras.tar.gz: 100%|██████████| 211k/211k [00:00<00:00, 258kB/s] \n",
            "/root/.cache/torch/mmf/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Starting checksum for extras.tar.gz]\n",
            "[ Checksum successful for extras.tar.gz]\n",
            "Unpacking extras.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/mmf/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:38<00:00, 10277.29it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib==3.3.4\n",
        "from mmf.common.registry import registry\n",
        "from mmf.models.mmbt import MMBT\n",
        "from mmf.utils.build import build_dataset\n",
        "from mmf.utils.env import setup_imports\n",
        "\n",
        "setup_imports()\n",
        "dataset = build_dataset(\"hateful_memes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANjYvkMuaC6g",
        "outputId": "0a59f428-e480-405a-e3b7-21b7a22142d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/mmbt/defaults.yaml\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 5000\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'training.max_updates=5000'])\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf_cli.run: \u001b[0mUsing seed 41100325\n",
            "\u001b[32m2021-12-12T00:15:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphtc5su9m\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 31.4kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp060rrp4s\n",
            "Downloading: 100% 570/570 [00:00<00:00, 704kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpthqjbnva\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.07MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9hcxidm4\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 4.92MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:15:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:15:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T00:15:42 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-12T00:15:42 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-12T00:15:42 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-12-12T00:15:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbw5lv4y3\n",
            "Downloading: 100% 440M/440M [00:09<00:00, 46.4MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100% 230M/230M [00:04<00:00, 48.3MB/s]\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2021-12-12T00:16:08 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:16:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:16:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\n",
            "\u001b[32m2021-12-12T00:17:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/5000, train/hateful_memes/cross_entropy: 0.7477, train/hateful_memes/cross_entropy/avg: 0.7477, train/total_loss: 0.7477, train/total_loss/avg: 0.7477, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 5000, lr: 0., ups: 1.08, time: 01m 33s 099ms, time_since_start: 01m 33s 153ms, eta: 01h 17m 19s 440ms\n",
            "\u001b[32m2021-12-12T00:19:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/5000, train/hateful_memes/cross_entropy: 0.7305, train/hateful_memes/cross_entropy/avg: 0.7391, train/total_loss: 0.7305, train/total_loss/avg: 0.7391, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 111ms, time_since_start: 03m 03s 264ms, eta: 01h 13m 18s 878ms\n",
            "\u001b[32m2021-12-12T00:20:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/5000, train/hateful_memes/cross_entropy: 0.7305, train/hateful_memes/cross_entropy/avg: 0.7220, train/total_loss: 0.7305, train/total_loss/avg: 0.7220, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 938ms, time_since_start: 04m 35s 202ms, eta: 01h 13m 14s 564ms\n",
            "\u001b[32m2021-12-12T00:22:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/5000, train/hateful_memes/cross_entropy: 0.6877, train/hateful_memes/cross_entropy/avg: 0.6677, train/total_loss: 0.6877, train/total_loss/avg: 0.6677, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 5000, lr: 0., ups: 1.12, time: 01m 29s 958ms, time_since_start: 06m 05s 161ms, eta: 01h 10m 08s 441ms\n",
            "\u001b[32m2021-12-12T00:23:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/5000, train/hateful_memes/cross_entropy: 0.7115, train/hateful_memes/cross_entropy/avg: 0.6764, train/total_loss: 0.7115, train/total_loss/avg: 0.6764, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 253ms, time_since_start: 07m 35s 415ms, eta: 01h 08m 50s 474ms\n",
            "\u001b[32m2021-12-12T00:25:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/5000, train/hateful_memes/cross_entropy: 0.6877, train/hateful_memes/cross_entropy/avg: 0.6759, train/total_loss: 0.6877, train/total_loss/avg: 0.6759, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 862ms, time_since_start: 09m 07s 278ms, eta: 01h 08m 30s 671ms\n",
            "\u001b[32m2021-12-12T00:26:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/5000, train/hateful_memes/cross_entropy: 0.6877, train/hateful_memes/cross_entropy/avg: 0.6378, train/total_loss: 0.6877, train/total_loss/avg: 0.6378, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 178ms, time_since_start: 10m 37s 456ms, eta: 01h 05m 43s 578ms\n",
            "\u001b[32m2021-12-12T00:28:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/5000, train/hateful_memes/cross_entropy: 0.6734, train/hateful_memes/cross_entropy/avg: 0.6229, train/total_loss: 0.6734, train/total_loss/avg: 0.6229, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 683ms, time_since_start: 12m 09s 139ms, eta: 01h 05m 16s 155ms\n",
            "\u001b[32m2021-12-12T00:29:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/5000, train/hateful_memes/cross_entropy: 0.6734, train/hateful_memes/cross_entropy/avg: 0.6053, train/total_loss: 0.6734, train/total_loss/avg: 0.6053, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 282ms, time_since_start: 13m 39s 421ms, eta: 01h 02m 44s 493ms\n",
            "\u001b[32m2021-12-12T00:31:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-12T00:31:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T00:31:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T00:31:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T00:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/5000, train/hateful_memes/cross_entropy: 0.5837, train/hateful_memes/cross_entropy/avg: 0.6032, train/total_loss: 0.5837, train/total_loss/avg: 0.6032, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 02m 01s 685ms, time_since_start: 15m 41s 107ms, eta: 01h 22m 30s 166ms\n",
            "\u001b[32m2021-12-12T00:31:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-12T00:31:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:31:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:31:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T00:31:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-12T00:31:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T00:32:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T00:32:12 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-12-12T00:32:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T00:32:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T00:32:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/5000, val/hateful_memes/cross_entropy: 0.7229, val/total_loss: 0.7229, val/hateful_memes/accuracy: 0.6148, val/hateful_memes/binary_f1: 0.3659, val/hateful_memes/roc_auc: 0.5955, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 5000, val_time: 01m 07s 621ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.595515\n",
            "\u001b[32m2021-12-12T00:34:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/5000, train/hateful_memes/cross_entropy: 0.5837, train/hateful_memes/cross_entropy/avg: 0.5909, train/total_loss: 0.5837, train/total_loss/avg: 0.5909, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 5000, lr: 0.00001, ups: 1.09, time: 01m 32s 395ms, time_since_start: 18m 21s 125ms, eta: 01h 01m 04s 677ms\n",
            "\u001b[32m2021-12-12T00:35:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/5000, train/hateful_memes/cross_entropy: 0.5190, train/hateful_memes/cross_entropy/avg: 0.5646, train/total_loss: 0.5190, train/total_loss/avg: 0.5646, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 078ms, time_since_start: 19m 51s 203ms, eta: 58m 01s 159ms\n",
            "\u001b[32m2021-12-12T00:37:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/5000, train/hateful_memes/cross_entropy: 0.5190, train/hateful_memes/cross_entropy/avg: 0.5474, train/total_loss: 0.5190, train/total_loss/avg: 0.5474, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 926ms, time_since_start: 21m 21s 130ms, eta: 56m 23s 860ms\n",
            "\u001b[32m2021-12-12T00:39:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/5000, train/hateful_memes/cross_entropy: 0.5190, train/hateful_memes/cross_entropy/avg: 0.5457, train/total_loss: 0.5190, train/total_loss/avg: 0.5457, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 930ms, time_since_start: 22m 53s 060ms, eta: 56m 05s 757ms\n",
            "\u001b[32m2021-12-12T00:40:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/5000, train/hateful_memes/cross_entropy: 0.5190, train/hateful_memes/cross_entropy/avg: 0.5258, train/total_loss: 0.5190, train/total_loss/avg: 0.5258, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 993ms, time_since_start: 24m 23s 054ms, eta: 53m 23s 314ms\n",
            "\u001b[32m2021-12-12T00:42:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/5000, train/hateful_memes/cross_entropy: 0.5047, train/hateful_memes/cross_entropy/avg: 0.5007, train/total_loss: 0.5047, train/total_loss/avg: 0.5007, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 666ms, time_since_start: 25m 54s 720ms, eta: 52m 49s 651ms\n",
            "\u001b[32m2021-12-12T00:43:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/5000, train/hateful_memes/cross_entropy: 0.5047, train/hateful_memes/cross_entropy/avg: 0.4777, train/total_loss: 0.5047, train/total_loss/avg: 0.4777, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 041ms, time_since_start: 27m 24s 762ms, eta: 50m 21s 875ms\n",
            "\u001b[32m2021-12-12T00:45:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/5000, train/hateful_memes/cross_entropy: 0.4685, train/hateful_memes/cross_entropy/avg: 0.4609, train/total_loss: 0.4685, train/total_loss/avg: 0.4609, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 165ms, time_since_start: 28m 54s 927ms, eta: 48m 54s 331ms\n",
            "\u001b[32m2021-12-12T00:46:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/5000, train/hateful_memes/cross_entropy: 0.4685, train/hateful_memes/cross_entropy/avg: 0.4384, train/total_loss: 0.4685, train/total_loss/avg: 0.4384, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 512ms, time_since_start: 30m 26s 439ms, eta: 48m 05s 116ms\n",
            "\u001b[32m2021-12-12T00:48:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-12T00:48:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T00:48:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T00:48:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T00:48:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, train/hateful_memes/cross_entropy: 0.4647, train/hateful_memes/cross_entropy/avg: 0.4186, train/total_loss: 0.4647, train/total_loss/avg: 0.4186, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 5000, lr: 0.00001, ups: 0.82, time: 02m 02s 973ms, time_since_start: 32m 29s 413ms, eta: 01h 02m 31s 929ms\n",
            "\u001b[32m2021-12-12T00:48:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-12T00:48:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T00:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T00:48:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-12T00:48:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T00:48:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T00:48:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-12-12T00:49:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T00:49:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T00:49:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, val/hateful_memes/cross_entropy: 1.3656, val/total_loss: 1.3656, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3322, val/hateful_memes/roc_auc: 0.6037, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 5000, val_time: 01m 06s 853ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.603662\n",
            "\u001b[32m2021-12-12T00:51:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/5000, train/hateful_memes/cross_entropy: 0.4086, train/hateful_memes/cross_entropy/avg: 0.4127, train/total_loss: 0.4086, train/total_loss/avg: 0.4127, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 925ms, time_since_start: 35m 07s 193ms, eta: 44m 41s 653ms\n",
            "\u001b[32m2021-12-12T00:52:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/5000, train/hateful_memes/cross_entropy: 0.3411, train/hateful_memes/cross_entropy/avg: 0.4012, train/total_loss: 0.3411, train/total_loss/avg: 0.4012, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 772ms, time_since_start: 36m 38s 966ms, eta: 43m 33s 316ms\n",
            "\u001b[32m2021-12-12T00:54:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/5000, train/hateful_memes/cross_entropy: 0.2947, train/hateful_memes/cross_entropy/avg: 0.3851, train/total_loss: 0.2947, train/total_loss/avg: 0.3851, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 208ms, time_since_start: 38m 09s 174ms, eta: 41m 17s 028ms\n",
            "\u001b[32m2021-12-12T00:55:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/5000, train/hateful_memes/cross_entropy: 0.2747, train/hateful_memes/cross_entropy/avg: 0.3744, train/total_loss: 0.2747, train/total_loss/avg: 0.3744, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 799ms, time_since_start: 39m 40s 974ms, eta: 40m 27s 375ms\n",
            "\u001b[32m2021-12-12T00:57:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/5000, train/hateful_memes/cross_entropy: 0.2472, train/hateful_memes/cross_entropy/avg: 0.3600, train/total_loss: 0.2472, train/total_loss/avg: 0.3600, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 003ms, time_since_start: 41m 10s 977ms, eta: 38m 08s 327ms\n",
            "\u001b[32m2021-12-12T00:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/5000, train/hateful_memes/cross_entropy: 0.1745, train/hateful_memes/cross_entropy/avg: 0.3479, train/total_loss: 0.1745, train/total_loss/avg: 0.3479, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 062ms, time_since_start: 42m 41s 040ms, eta: 36m 38s 250ms\n",
            "\u001b[32m2021-12-12T01:00:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/5000, train/hateful_memes/cross_entropy: 0.1612, train/hateful_memes/cross_entropy/avg: 0.3351, train/total_loss: 0.1612, train/total_loss/avg: 0.3351, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 428ms, time_since_start: 44m 12s 468ms, eta: 35m 38s 594ms\n",
            "\u001b[32m2021-12-12T01:01:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/5000, train/hateful_memes/cross_entropy: 0.1300, train/hateful_memes/cross_entropy/avg: 0.3233, train/total_loss: 0.1300, train/total_loss/avg: 0.3233, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 096ms, time_since_start: 45m 42s 565ms, eta: 33m 35s 820ms\n",
            "\u001b[32m2021-12-12T01:03:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/5000, train/hateful_memes/cross_entropy: 0.1240, train/hateful_memes/cross_entropy/avg: 0.3147, train/total_loss: 0.1240, train/total_loss/avg: 0.3147, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 130ms, time_since_start: 47m 12s 696ms, eta: 32m 04s 925ms\n",
            "\u001b[32m2021-12-12T01:04:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-12T01:04:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:05:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:05:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:05:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/5000, train/hateful_memes/cross_entropy: 0.1100, train/hateful_memes/cross_entropy/avg: 0.3049, train/total_loss: 0.1100, train/total_loss/avg: 0.3049, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 5000, lr: 0.00001, ups: 0.81, time: 02m 04s 471ms, time_since_start: 49m 17s 167ms, eta: 42m 11s 748ms\n",
            "\u001b[32m2021-12-12T01:05:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-12T01:05:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:05:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:05:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T01:05:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-12T01:05:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T01:05:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:05:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:06:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:06:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/5000, val/hateful_memes/cross_entropy: 2.0220, val/total_loss: 2.0220, val/hateful_memes/accuracy: 0.6630, val/hateful_memes/binary_f1: 0.3546, val/hateful_memes/roc_auc: 0.5754, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 5000, val_time: 43s 553ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.603662\n",
            "\u001b[32m2021-12-12T01:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/5000, train/hateful_memes/cross_entropy: 0.0742, train/hateful_memes/cross_entropy/avg: 0.2951, train/total_loss: 0.0742, train/total_loss/avg: 0.2951, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 769ms, time_since_start: 51m 31s 491ms, eta: 29m 13s 939ms\n",
            "\u001b[32m2021-12-12T01:09:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/5000, train/hateful_memes/cross_entropy: 0.0446, train/hateful_memes/cross_entropy/avg: 0.2860, train/total_loss: 0.0446, train/total_loss/avg: 0.2860, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 874ms, time_since_start: 53m 03s 366ms, eta: 28m 01s 862ms\n",
            "\u001b[32m2021-12-12T01:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/5000, train/hateful_memes/cross_entropy: 0.0421, train/hateful_memes/cross_entropy/avg: 0.2774, train/total_loss: 0.0421, train/total_loss/avg: 0.2774, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 5000, lr: 0.00001, ups: 1.12, time: 01m 29s 936ms, time_since_start: 54m 33s 303ms, eta: 25m 54s 917ms\n",
            "\u001b[32m2021-12-12T01:12:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/5000, train/hateful_memes/cross_entropy: 0.0334, train/hateful_memes/cross_entropy/avg: 0.2693, train/total_loss: 0.0334, train/total_loss/avg: 0.2693, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 5000, lr: 0.00001, ups: 1.11, time: 01m 30s 063ms, time_since_start: 56m 03s 367ms, eta: 24m 25s 511ms\n",
            "\u001b[32m2021-12-12T01:13:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/5000, train/hateful_memes/cross_entropy: 0.0334, train/hateful_memes/cross_entropy/avg: 0.2627, train/total_loss: 0.0334, train/total_loss/avg: 0.2627, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 5000, lr: 0.00001, ups: 1.10, time: 01m 31s 853ms, time_since_start: 57m 35s 220ms, eta: 23m 21s 221ms\n",
            "\u001b[32m2021-12-12T01:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/5000, train/hateful_memes/cross_entropy: 0.0293, train/hateful_memes/cross_entropy/avg: 0.2556, train/total_loss: 0.0293, train/total_loss/avg: 0.2556, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 012ms, time_since_start: 59m 05s 233ms, eta: 21m 21s 602ms\n",
            "\u001b[32m2021-12-12T01:16:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/5000, train/hateful_memes/cross_entropy: 0.0216, train/hateful_memes/cross_entropy/avg: 0.2488, train/total_loss: 0.0216, train/total_loss/avg: 0.2488, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 068ms, time_since_start: 01h 35s 301ms, eta: 19m 50s 798ms\n",
            "\u001b[32m2021-12-12T01:18:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/5000, train/hateful_memes/cross_entropy: 0.0135, train/hateful_memes/cross_entropy/avg: 0.2423, train/total_loss: 0.0135, train/total_loss/avg: 0.2423, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 808ms, time_since_start: 01h 02m 07s 110ms, eta: 18m 40s 427ms\n",
            "\u001b[32m2021-12-12T01:19:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/5000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.2361, train/total_loss: 0.0056, train/total_loss/avg: 0.2361, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 071ms, time_since_start: 01h 03m 37s 182ms, eta: 16m 47s 635ms\n",
            "\u001b[32m2021-12-12T01:21:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-12T01:21:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:21:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:21:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:21:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/5000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.2304, train/total_loss: 0.0056, train/total_loss/avg: 0.2304, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 5000, lr: 0., ups: 0.81, time: 02m 04s 568ms, time_since_start: 01h 05m 41s 750ms, eta: 21m 06s 863ms\n",
            "\u001b[32m2021-12-12T01:21:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-12T01:21:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T01:21:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-12T01:21:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T01:21:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:22:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:22:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/5000, val/hateful_memes/cross_entropy: 2.4482, val/total_loss: 2.4482, val/hateful_memes/accuracy: 0.6426, val/hateful_memes/binary_f1: 0.3545, val/hateful_memes/roc_auc: 0.5855, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 5000, val_time: 44s 741ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.603662\n",
            "\u001b[32m2021-12-12T01:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/5000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.2248, train/total_loss: 0.0038, train/total_loss/avg: 0.2248, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 769ms, time_since_start: 01h 07m 57s 263ms, eta: 13m 50s 810ms\n",
            "\u001b[32m2021-12-12T01:25:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/5000, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.2195, train/total_loss: 0.0034, train/total_loss/avg: 0.2195, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 027ms, time_since_start: 01h 09m 27s 291ms, eta: 12m 12s 466ms\n",
            "\u001b[32m2021-12-12T01:27:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/5000, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.2156, train/total_loss: 0.0034, train/total_loss/avg: 0.2156, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 682ms, time_since_start: 01h 10m 58s 973ms, eta: 10m 52s 688ms\n",
            "\u001b[32m2021-12-12T01:28:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/5000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2108, train/total_loss: 0.0028, train/total_loss/avg: 0.2108, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 223ms, time_since_start: 01h 12m 29s 197ms, eta: 09m 10s 541ms\n",
            "\u001b[32m2021-12-12T01:30:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/5000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.2061, train/total_loss: 0.0026, train/total_loss/avg: 0.2061, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 302ms, time_since_start: 01h 13m 59s 499ms, eta: 07m 39s 188ms\n",
            "\u001b[32m2021-12-12T01:31:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/5000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.2016, train/total_loss: 0.0025, train/total_loss/avg: 0.2016, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 829ms, time_since_start: 01h 15m 31s 328ms, eta: 06m 13s 560ms\n",
            "\u001b[32m2021-12-12T01:33:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/5000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1973, train/total_loss: 0.0023, train/total_loss/avg: 0.1973, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 103ms, time_since_start: 01h 17m 01s 432ms, eta: 04m 34s 906ms\n",
            "\u001b[32m2021-12-12T01:34:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/5000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1932, train/total_loss: 0.0022, train/total_loss/avg: 0.1932, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 5000, lr: 0., ups: 1.10, time: 01m 31s 639ms, time_since_start: 01h 18m 33s 072ms, eta: 03m 06s 395ms\n",
            "\u001b[32m2021-12-12T01:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/5000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1893, train/total_loss: 0.0021, train/total_loss/avg: 0.1893, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 5000, lr: 0., ups: 1.11, time: 01m 30s 016ms, time_since_start: 01h 20m 03s 089ms, eta: 01m 31s 546ms\n",
            "\u001b[32m2021-12-12T01:37:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-12-12T01:37:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:37:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:38:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:38:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/5000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1855, train/total_loss: 0.0017, train/total_loss/avg: 0.1855, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 5000, lr: 0., ups: 0.83, time: 02m 01s 484ms, time_since_start: 01h 22m 04s 573ms, eta: 0ms\n",
            "\u001b[32m2021-12-12T01:38:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-12-12T01:38:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:38:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:38:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-12-12T01:38:21 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2021-12-12T01:38:21 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T01:38:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-12-12T01:38:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-12-12T01:38:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-12-12T01:38:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/5000, val/hateful_memes/cross_entropy: 2.5347, val/total_loss: 2.5347, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.3625, val/hateful_memes/roc_auc: 0.5890, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 5000, val_time: 44s 445ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.603662\n",
            "\u001b[32m2021-12-12T01:38:57 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-12-12T01:38:57 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-12-12T01:38:57 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2021-12-12T01:39:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-12-12T01:39:09 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n",
            "\u001b[32m2021-12-12T01:39:09 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n",
            "\u001b[32m2021-12-12T01:39:09 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n",
            "\u001b[32m2021-12-12T01:39:11 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2021-12-12T01:39:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:39:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-12-12T01:39:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 63/63 [00:25<00:00,  2.47it/s]\n",
            "\u001b[32m2021-12-12T01:39:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n",
            "\u001b[32m2021-12-12T01:39:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2021-12-12T01:39:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/5000, test/hateful_memes/cross_entropy: 1.2713, test/total_loss: 1.2713, test/hateful_memes/accuracy: 0.6485, test/hateful_memes/binary_f1: 0.4156, test/hateful_memes/roc_auc: 0.6422\n",
            "\u001b[32m2021-12-12T01:39:37 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 23m 28s 969ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!mmf_run config=projects/hateful_memes/configs/mmbt/defaults.yaml model=mmbt dataset=hateful_memes training.max_updates=5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE2XH-68VQNL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDhffPnizrL5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Meme.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kgp5aYo_ULL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/mmf.git\n",
        "%cd mmf\n",
        "!pip install --editable ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3oft7p_Z0I",
        "outputId": "7622c954-cf7c-4e13-ec16-266551334677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ5ytYJO_bos"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/DL_Group_Project/hm_convert.py\" /content/mmf/mmf_cli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6d2kpez_dcC"
      },
      "outputs": [],
      "source": [
        "!mmf_convert_hm --zip_file='/content/drive/MyDrive/DL_Group_Project/hateful_memes.zip' --password='' --bypass_checksum=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxS1J6dT_jCL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from mmf.models.mmbt import MMBT\n",
        "\n",
        "model = MMBT.from_pretrained(\"mmbt.hateful_memes.images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE78mU4SSFMr"
      },
      "outputs": [],
      "source": [
        "# Get prediction on Memotion_Memes dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "ids = []\n",
        "images_url = []\n",
        "texts = []\n",
        "predictions = []\n",
        "confidences = []\n",
        "\n",
        "info = pd.read_csv(\"/content/drive/MyDrive/DL_Group_Project/reference.csv\")\n",
        "txt_info = pd.read_csv(\"/content/drive/MyDrive/DL_Group_Project/labels.csv\")\n",
        "ids = info.image_name.to_list()\n",
        "images_url = info.image_url.to_list()\n",
        "texts = txt_info.text_corrected.to_list()\n",
        "\n",
        "for i in range(len(ids)):\n",
        "    image_url = images_url[i] #@param {type:\"string\"}\n",
        "    text = texts[i] #@param {type: \"string\"}\n",
        "    try:\n",
        "        output = model.classify(image_url, text)\n",
        "        predictions.append(output[\"label\"])\n",
        "        confidences.append(output[\"confidence\"])\n",
        "    except:\n",
        "        predictions.append(0)\n",
        "        confidences.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKmhbN05UPar"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# format pseudo-labeled data and add to hateful_memes dataset for training\n",
        "info_list = []\n",
        "info_list1 = [] #confident prediction\n",
        "\n",
        "id_start = 1198764\n",
        "# for image file name replacement\n",
        "name_change = {}\n",
        "\n",
        "for i in range(len(ids)):\n",
        "    new_info = {}\n",
        "    id_start += 1\n",
        "    new_info['id'] = str(id_start)\n",
        "    postfix = images_url[i].split('.')[-1]\n",
        "    new_info['img'] = \"img/\" + str(id_start) + \".png\"\n",
        "    name_change[ids[i] + '.' + postfix] = str(id_start) + '.png'\n",
        "    new_info['label'] = predictions[i]\n",
        "    new_info['text'] = texts[i]\n",
        "    info_list.append(new_info)\n",
        "\n",
        "    # change label according to confidence\n",
        "    if predictions[i] == 0 and confidences[i] < 0.8:\n",
        "        new_info['label'] = 1\n",
        "        info_list1.append(new_info)\n",
        "    elif predictions[i] == 1 and confidences[i] < 0.5:\n",
        "        new_info['label'] = 0\n",
        "        info_list1.append(new_info)\n",
        "    else:\n",
        "        info_list1.append(new_info)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/DL_Group_Project/memotion_pred.jsonl\", 'w') as outfile:\n",
        "    for entry in info_list:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "with open(\"/content/drive/MyDrive/DL_Group_Project/memotion_pred1.jsonl\", 'w') as outfile:\n",
        "    for entry in info_list1:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\t\t\n",
        "# save information for image filename change to json\n",
        "import json\n",
        "with open('/content/drive/MyDrive/DL_Group_Project/memotion_name_change.json', 'w') as outfile:\n",
        "    json.dump(name_change, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evRC1O8x_ogL"
      },
      "outputs": [],
      "source": [
        "# Get prediction on Reddit_Memes dataset\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "ids = []\n",
        "images_url = []\n",
        "texts = []\n",
        "predictions = []\n",
        "confidences = []\n",
        "# get texts from json\n",
        "with open (\"/content/drive/MyDrive/DL_Group_Project/reddit_memes_db.json\", 'r', encoding=\"utf8\") as f:\n",
        "    info = json.loads(f.read())\n",
        "\n",
        "for i in range(1, 3227):\n",
        "    ids.append(info['_default'][str(i)]['id'])\n",
        "    images_url.append(info['_default'][str(i)]['media'])\n",
        "    texts.append(info['_default'][str(i)]['title'])\n",
        "\n",
        "for i in range(3226):\n",
        "    image_url = images_url[i] #@param {type:\"string\"}\n",
        "    text = texts[i] #@param {type: \"string\"}\n",
        "    try:\n",
        "        output = model.classify(image_url, text)\n",
        "        predictions.append(output[\"label\"])\n",
        "        confidences.append(output[\"confidence\"])\n",
        "    except:\n",
        "        predictions.append(0)\n",
        "        confidences.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM1db4GT_ruo"
      },
      "outputs": [],
      "source": [
        "# format pseudo-labeled data and add to hateful_memes dataset for training\n",
        "info_list = []\n",
        "info_list1 = [] #confident prediction\n",
        "\n",
        "id_start = 198764\n",
        "# for image file name replacement\n",
        "name_change = {}\n",
        "\n",
        "for i in range(3226):\n",
        "    new_info = {}\n",
        "    id_start += 1\n",
        "    new_info['id'] = str(id_start)\n",
        "    postfix = images_url[i].split('.')[-1]\n",
        "    new_info['img'] = \"img/\" + str(id_start) + \".png\"\n",
        "    name_change[ids[i] + '.' + postfix] = str(id_start) + '.png'\n",
        "    new_info['label'] = predictions[i]\n",
        "    new_info['text'] = texts[i]\n",
        "    info_list.append(new_info)\n",
        "\n",
        "    # change label according to confidence\n",
        "    if predictions[i] == 0 and confidences[i] < 0.9:\n",
        "        new_info['label'] = 1\n",
        "        info_list1.append(new_info)\n",
        "    elif predictions[i] == 1 and confidences[i] < 0.5:\n",
        "        new_info['label'] = 0\n",
        "        info_list1.append(new_info)\n",
        "    else:\n",
        "        info_list1.append(new_info)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/DL_Group_Project/train.jsonl\", 'w') as outfile:\n",
        "    for entry in info_list:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "with open(\"/content/drive/MyDrive/DL_Group_Project/train1.jsonl\", 'w') as outfile:\n",
        "    for entry in info_list1:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\t\t\n",
        "# save information for image filename change to json\n",
        "import json\n",
        "with open('/content/drive/MyDrive/DL_Group_Project/name_change.json', 'w') as outfile:\n",
        "    json.dump(name_change, outfile)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Self_Training_Pseudo_Labels_Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}